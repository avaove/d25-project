{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_comments Data Loading + Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_reader = pd.read_csv(\"data/text_comments.csv\", iterator=True, chunksize=1000000,lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_comments.csv is very large, so going to split it into 2 time-based dataframes: before and after Feb 1, 2020 \n",
    "# Each dataframe will have id, linkid, body, and created_utc\n",
    "# Bot Detection: The dataframes will not contain author names but we will also remove rows with bot or mod in the author names\n",
    "# Removed Comments: If a comment has body [deleted] or [removed] then, we remove those as well\n",
    "# It's easier to deal with 2 smaller datasets but if necessary, we can concatenate the pre and post covid dataframes later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.DataFrame() # Will temporarily store dataframes for each chunk (not changing the data) \n",
    "lst = [] # will store dataframes after they have been pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in text_reader: \n",
    "    # Add each chunk of txt_comments to a df and append it to a list\n",
    "    temp = pd.DataFrame()\n",
    "    temp = chunk[[\"id\", \"link_id\", \"author\", \"body\", \"created_utc\"]]\n",
    "    lst.append(temp)\n",
    "    # time: 5m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_shape = [] # will store how many rows are in each small dataframe - so that we can keep track that we're not missing rows by the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# Modify each dataframe in lst by removing deleted comments, removing comments from bots, dropping the author column, and changing created_utc into int type\n",
    "\n",
    "for i in range(len(lst)):\n",
    "    print(i)\n",
    "\n",
    "    # remove usernames that indicate it's a bot and comments that are [removed] and [deleted]\n",
    "    lst[i] = lst[i][~((lst[i][\"body\"] == \"[removed]\") | (lst[i][\"body\"] == \"[deleted]\") | lst[i][\"author\"].str.lower().str.contains(\"bot|mod\"))]\n",
    "    # we're not keeping author column\n",
    "    lst[i] = lst[i].drop(columns=['author'])\n",
    "    # transform date into int\n",
    "    lst[i]['created_utc'] = lst[i]['created_utc'].astype(int)\n",
    "    lst_shape.append(lst[i].shape[0])\n",
    "    # time: 10m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36449566"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lst_shape) # the total number of comments after pruning: 36449566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes that will only have comments split based on Feb 01, 2022 UTC\n",
    "# start of the pandemic: February 1, 2020 \n",
    "# created_at is a UNIX timestamp. Feb 1, 2020 00:00:00 UTC = 1580515200\n",
    "\n",
    "lst_precovid = []\n",
    "lst_postcovid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "# For each dataframe in lst, create two dataframes that has comments based on dates and append it to the correct list\n",
    "\n",
    "for i in range(len(lst)): \n",
    "    print(i)\n",
    "\n",
    "    # Pre-covid \n",
    "    lst_precovid.append(lst[i][lst[i][\"created_utc\"] < 1580515200])\n",
    "    # Post-covid \n",
    "    lst_postcovid.append(lst[i][lst[i][\"created_utc\"] >= 1580515200])\n",
    "\n",
    "    # time: 7-10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the dataframes that have comments before Feb 1, 2020 UTC into 1 dataframe\n",
    "df_comments_precovid = pd.concat(lst_precovid) # time: 2-3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the dataframes that have comments on or after Feb 1, 2020 UTC into 1 dataframe\n",
    "df_comments_postcovid = pd.concat(lst_postcovid) # time: 3min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link_id</th>\n",
       "      <th>body</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3058</th>\n",
       "      <td>t1_fg500j1</td>\n",
       "      <td>t3_ewseet</td>\n",
       "      <td>I really love how precise she is with makeup a...</td>\n",
       "      <td>1580514977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3062</th>\n",
       "      <td>t1_fd2eb6n</td>\n",
       "      <td>t3_ejq7ek</td>\n",
       "      <td>That part of the whole story is extremely susp...</td>\n",
       "      <td>1578145985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3069</th>\n",
       "      <td>t1_fd3xequ</td>\n",
       "      <td>t3_ejuxan</td>\n",
       "      <td>Prep: If it's a constant-work period, I let pe...</td>\n",
       "      <td>1578162447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3070</th>\n",
       "      <td>t1_fd4llkb</td>\n",
       "      <td>t3_ek1pnm</td>\n",
       "      <td>Can you repost linking the actual video and no...</td>\n",
       "      <td>1578168690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3076</th>\n",
       "      <td>t1_fd63yhw</td>\n",
       "      <td>t3_ek3ifu</td>\n",
       "      <td>This gave me weird al vibes and I’m living for...</td>\n",
       "      <td>1578185200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40721545</th>\n",
       "      <td>t1_ew3s0p8</td>\n",
       "      <td>t3_cmccmo</td>\n",
       "      <td>Did people forget Finn had to face Corbin, Jin...</td>\n",
       "      <td>1565092947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40721546</th>\n",
       "      <td>t1_ennpzrt</td>\n",
       "      <td>t3_bp2f3w</td>\n",
       "      <td>I hope</td>\n",
       "      <td>1557949685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40721547</th>\n",
       "      <td>t1_f02psre</td>\n",
       "      <td>t3_d3f1tn</td>\n",
       "      <td>I will literally cry if you're right dude xD L...</td>\n",
       "      <td>1568335635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40721548</th>\n",
       "      <td>t1_f5t4bhw</td>\n",
       "      <td>t3_dp6jz9</td>\n",
       "      <td>Wow.  That's incredibly hostile for no reason.</td>\n",
       "      <td>1572450141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40721549</th>\n",
       "      <td>t1_errythg</td>\n",
       "      <td>t3_c3ad9x</td>\n",
       "      <td>Yes and it's insanely unrealistic. But, school...</td>\n",
       "      <td>1561190159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13635058 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id    link_id  \\\n",
       "3058      t1_fg500j1  t3_ewseet   \n",
       "3062      t1_fd2eb6n  t3_ejq7ek   \n",
       "3069      t1_fd3xequ  t3_ejuxan   \n",
       "3070      t1_fd4llkb  t3_ek1pnm   \n",
       "3076      t1_fd63yhw  t3_ek3ifu   \n",
       "...              ...        ...   \n",
       "40721545  t1_ew3s0p8  t3_cmccmo   \n",
       "40721546  t1_ennpzrt  t3_bp2f3w   \n",
       "40721547  t1_f02psre  t3_d3f1tn   \n",
       "40721548  t1_f5t4bhw  t3_dp6jz9   \n",
       "40721549  t1_errythg  t3_c3ad9x   \n",
       "\n",
       "                                                       body  created_utc  \n",
       "3058      I really love how precise she is with makeup a...   1580514977  \n",
       "3062      That part of the whole story is extremely susp...   1578145985  \n",
       "3069      Prep: If it's a constant-work period, I let pe...   1578162447  \n",
       "3070      Can you repost linking the actual video and no...   1578168690  \n",
       "3076      This gave me weird al vibes and I’m living for...   1578185200  \n",
       "...                                                     ...          ...  \n",
       "40721545  Did people forget Finn had to face Corbin, Jin...   1565092947  \n",
       "40721546                                             I hope   1557949685  \n",
       "40721547  I will literally cry if you're right dude xD L...   1568335635  \n",
       "40721548     Wow.  That's incredibly hostile for no reason.   1572450141  \n",
       "40721549  Yes and it's insanely unrealistic. But, school...   1561190159  \n",
       "\n",
       "[13635058 rows x 4 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments_precovid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22814508, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments_postcovid.shape # (22814508, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the sizes are the same: 40721549 index. 40721550 rows total\n",
    "# 22814508+13635058 = sum(lst_shape) = 36449566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_comments_pre.to_csv('data/df_comments_pre.csv') # time: \n",
    "# df_comments_postcovid.to_csv('data/df_comments_post.csv') # time: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text_submissions Data Loading + Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\farih\\AppData\\Local\\Temp\\ipykernel_11164\\2287351077.py:1: DtypeWarning: Columns (2,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  text_submissions_df = pd.read_csv(\"data/text_submissions.csv\")\n"
     ]
    }
   ],
   "source": [
    "text_submissions_df = pd.read_csv(\"data/text_submissions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>is_self</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t3_npxigk</td>\n",
       "      <td>All_Consuming_Void</td>\n",
       "      <td>1622563615</td>\n",
       "      <td>self.BeautyGuruChatter</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>Hyram launches his own brand</td>\n",
       "      <td>BeautyGuruChatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t3_nqj6bf</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>1622631621</td>\n",
       "      <td>self.BeautyGuruChatter</td>\n",
       "      <td>True</td>\n",
       "      <td>38.0</td>\n",
       "      <td>What are the influencers trying to influence y...</td>\n",
       "      <td>What I'm not gonna buy Wednesday - Anti-haul</td>\n",
       "      <td>BeautyGuruChatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t3_nk0btr</td>\n",
       "      <td>barrahhhh</td>\n",
       "      <td>1621869439</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>False</td>\n",
       "      <td>144.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Plouise goes off in facebook group for 'bullying'</td>\n",
       "      <td>BeautyGuruChatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t3_nrbybs</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1622722260</td>\n",
       "      <td>self.BeautyGuruChatter</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Is youtube algorithm against Susan Yara? She g...</td>\n",
       "      <td>BeautyGuruChatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t3_nl0ebd</td>\n",
       "      <td>carlosShook</td>\n",
       "      <td>1621977767</td>\n",
       "      <td>vm.tiktok.com</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sephora steals concept from Huntr Faulknr afte...</td>\n",
       "      <td>BeautyGuruChatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496175</th>\n",
       "      <td>t3_ae5pzt</td>\n",
       "      <td>middlefinger22</td>\n",
       "      <td>1547030976</td>\n",
       "      <td>self.yakuzagames</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>And they won't have so much changes as Kiwami2.</td>\n",
       "      <td>Why it takes so long to release 3, 4 and 5 on ...</td>\n",
       "      <td>yakuzagames</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496176</th>\n",
       "      <td>t3_cer9jj</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>1563449362</td>\n",
       "      <td>self.nrl</td>\n",
       "      <td>True</td>\n",
       "      <td>15.0</td>\n",
       "      <td>|NRL|                                         ...</td>\n",
       "      <td>Round 18: Broncos vs Bulldogs | Post Match Dis...</td>\n",
       "      <td>nrl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496177</th>\n",
       "      <td>t3_bqidb1</td>\n",
       "      <td>nuke8960</td>\n",
       "      <td>1558280683</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>False</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gotta spread the word</td>\n",
       "      <td>BirdsArentReal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496178</th>\n",
       "      <td>t3_cqvbwk</td>\n",
       "      <td>anon3212</td>\n",
       "      <td>1565900167</td>\n",
       "      <td>self.ShadowBan</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Am I shadowbanned?</td>\n",
       "      <td>Test</td>\n",
       "      <td>ShadowBan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496179</th>\n",
       "      <td>t3_efamy2</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1577240277</td>\n",
       "      <td>clips.twitch.tv</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Greeks flirts with a girl</td>\n",
       "      <td>LivestreamFail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3496180 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id              author created_utc                  domain  \\\n",
       "0        t3_npxigk  All_Consuming_Void  1622563615  self.BeautyGuruChatter   \n",
       "1        t3_nqj6bf       AutoModerator  1622631621  self.BeautyGuruChatter   \n",
       "2        t3_nk0btr           barrahhhh  1621869439              reddit.com   \n",
       "3        t3_nrbybs           [deleted]  1622722260  self.BeautyGuruChatter   \n",
       "4        t3_nl0ebd         carlosShook  1621977767           vm.tiktok.com   \n",
       "...            ...                 ...         ...                     ...   \n",
       "3496175  t3_ae5pzt      middlefinger22  1547030976        self.yakuzagames   \n",
       "3496176  t3_cer9jj       AutoModerator  1563449362                self.nrl   \n",
       "3496177  t3_bqidb1            nuke8960  1558280683               i.redd.it   \n",
       "3496178  t3_cqvbwk            anon3212  1565900167          self.ShadowBan   \n",
       "3496179  t3_efamy2           [deleted]  1577240277         clips.twitch.tv   \n",
       "\n",
       "        is_self  score                                           selftext  \\\n",
       "0          True    0.0                                          [removed]   \n",
       "1          True   38.0  What are the influencers trying to influence y...   \n",
       "2         False  144.0                                                NaN   \n",
       "3          True    2.0                                          [deleted]   \n",
       "4         False    0.0                                                NaN   \n",
       "...         ...    ...                                                ...   \n",
       "3496175    True    1.0    And they won't have so much changes as Kiwami2.   \n",
       "3496176    True   15.0  |NRL|                                         ...   \n",
       "3496177   False   35.0                                                NaN   \n",
       "3496178    True    1.0                                 Am I shadowbanned?   \n",
       "3496179   False    1.0                                          [deleted]   \n",
       "\n",
       "                                                     title          subreddit  \n",
       "0                             Hyram launches his own brand  BeautyGuruChatter  \n",
       "1             What I'm not gonna buy Wednesday - Anti-haul  BeautyGuruChatter  \n",
       "2        Plouise goes off in facebook group for 'bullying'  BeautyGuruChatter  \n",
       "3        Is youtube algorithm against Susan Yara? She g...  BeautyGuruChatter  \n",
       "4        Sephora steals concept from Huntr Faulknr afte...  BeautyGuruChatter  \n",
       "...                                                    ...                ...  \n",
       "3496175  Why it takes so long to release 3, 4 and 5 on ...        yakuzagames  \n",
       "3496176  Round 18: Broncos vs Bulldogs | Post Match Dis...                nrl  \n",
       "3496177                              Gotta spread the word     BirdsArentReal  \n",
       "3496178                                               Test          ShadowBan  \n",
       "3496179                          Greeks flirts with a girl     LivestreamFail  \n",
       "\n",
       "[3496180 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_submissions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that aren't useful for us to make the dataframes smaller\n",
    "\n",
    "text_submissions_df=text_submissions_df.drop(columns=['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop this row because it has an invalid date and other columns are nan\n",
    "text_submissions_df=text_submissions_df[~(text_submissions_df['created_utc'] == \"CPTSD\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_submissions_df['created_utc'] = text_submissions_df['created_utc'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of the pandemic as of February 1, 2020\n",
    "# created_at is a UNIX timestamp. Feb 1, 2020 00:00:00 UTC = 1580515200\n",
    "text_submissions_df_pre = text_submissions_df[text_submissions_df[\"created_utc\"] < 1580515200]\n",
    "text_submissions_df_post = text_submissions_df[text_submissions_df[\"created_utc\"] >= 1580515200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Q): Bot Detection not necessary for Posts? just for text_comments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the 4 updated cvs which have cleaned up the data and split the comments and submissions csv files into 2 \n",
    "# because they are smaller files, thus easier to load up and deal with\n",
    "# text_submissions_df_pre.to_csv('data/text_submissions_pre.csv')\n",
    "# text_submissions_df_post.to_csv(\"data/text_submissions_post.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "908f734d8ce94021814a9781fc9ca0ce65be7da4bec4c909ba0a7fc993e0086a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
