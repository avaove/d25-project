{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title of the proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion?text=I+feel+a+bit+let+down\n",
    "# from transformers import pipeline\n",
    "\n",
    "# from tqdm import tqdm # prgress bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Comments and Submissions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 512. KiB for an array with shape (65536,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\farih\\Documents\\University Y5\\Fall 2023\\CSCD25\\d25-project\\copy.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/farih/Documents/University%20Y5/Fall%202023/CSCD25/d25-project/copy.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m text_submissions_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mdata/text_submissions.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\farih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\farih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\farih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\farih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\farih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\farih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\farih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\farih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\farih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\farih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\farih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1183\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\farih\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1744\u001b[0m, in \u001b[0;36mpandas._libs.parsers._try_int64\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 512. KiB for an array with shape (65536,) and data type int64"
     ]
    }
   ],
   "source": [
    "text_submissions_df = pd.read_csv(\"data/text_submissions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_comments_df = pd.read_csv(\"data/text_comments.csv\", iterator=True, chunksize=1000000,lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_comments.csv is very large, so going to split it into 2 time-based dataframes: before and after Feb 1, 2020 \n",
    "# Each dataframe will have id, linkid, body, and created_utc\n",
    "# Bot Detection: The dataframes will not contain author names but we will also remove rows with bot or mod in the author names\n",
    "# Removed Comments: If a comment has body [deleted] or [removed] then, we remove those as well\n",
    "# It's easier to deal with 2 smaller datasets but if necessary, we can concatenate the pre and post covid dataframes later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "temp = pd.DataFrame() # Will temporarily store dataframes for each chunk (not changing the data) \n",
    "lst = [] # will store dataframes after they have been pruned\n",
    "count = 0\n",
    "for chunk in text_comments_df: \n",
    "    print(count)\n",
    "    # Add each chunk of txt_comments to a df and append it to a list\n",
    "    temp = pd.DataFrame()\n",
    "    temp = chunk[[\"id\", \"link_id\", \"author\", \"body\", \"created_utc\"]]\n",
    "    lst.append(temp)\n",
    "    count+=1\n",
    "    # time: 5m-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_shape = [] # will store how many rows are in each small dataframe - so that we can keep track that we're not missing rows by the end\n",
    "\n",
    "# Modify each dataframe in lst by removing deleted comments, removing comments from bots, dropping the author column, and changing created_utc into int type\n",
    "for i in range(len(lst)):\n",
    "    print(i, end=' ')\n",
    "\n",
    "    # remove usernames that indicate it's a bot and comments that are [removed] and [deleted]\n",
    "    lst[i] = lst[i][~((lst[i][\"body\"] == \"[removed]\") | (lst[i][\"body\"] == \"[deleted]\") | lst[i][\"author\"].str.lower().str.contains(\"bot|mod\"))]\n",
    "    # we're not keeping author column\n",
    "    lst[i] = lst[i].drop(columns=['author'])\n",
    "    # transform date into int\n",
    "    lst[i]['created_utc'] = lst[i]['created_utc'].astype(int)\n",
    "    lst_shape.append(lst[i].shape[0])\n",
    "    # time: 10m\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(lst_shape) # the total number of comments after pruning: 36449566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes that will only have comments split based on Feb 01, 2022 UTC\n",
    "# start of the pandemic: February 1, 2020 \n",
    "# created_at is a UNIX timestamp. Feb 1, 2020 00:00:00 UTC = 1580515200\n",
    "\n",
    "lst_precovid, lst_postcovid = [], []\n",
    "start_of_covid = 1580515200\n",
    "\n",
    "# For each dataframe in lst, create two dataframes that has comments based on dates and append it to the correct list\n",
    "for i in range(len(lst)): \n",
    "    print(i)\n",
    "\n",
    "    # Pre-covid \n",
    "    lst_precovid.append(lst[i][lst[i][\"created_utc\"] < start_of_covid])\n",
    "    # Post-covid \n",
    "    lst_postcovid.append(lst[i][lst[i][\"created_utc\"] >= start_of_covid])\n",
    "\n",
    "    # time: 7-10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the dataframes that have comments before Feb 1, 2020 UTC into 1 dataframe\n",
    "df_comments_precovid = pd.concat(lst_precovid) # time: 2-3m\n",
    "\n",
    "# Concatenate all the dataframes that have comments on or after Feb 1, 2020 UTC into 1 dataframe\n",
    "df_comments_postcovid = pd.concat(lst_postcovid) # time: 3min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_comments_precovid\n",
    "# df_comments_postcovid.shape # (22814508, 4)\n",
    "# check the sizes are the same: 40721549 index. 40721550 rows total\n",
    "# 22814508+13635058 = sum(lst_shape) = 36449566\n",
    "# df_comments_pre.to_csv('data/df_comments_pre.csv') # time: \n",
    "# df_comments_postcovid.to_csv('data/df_comments_post.csv') # time: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns that aren't useful for us in order to make the dataframes smaller\n",
    "text_submissions_df=text_submissions_df.drop(columns=['score'])\n",
    "\n",
    "# Drop submissions with title or text that is empty or na\n",
    "text_submissions_df.drop(text_submissions_df[text_submissions_df['title'].isna() | (text_submissions_df['title'] == '')].index, inplace=True)\n",
    "text_submissions_df.drop(text_submissions_df[text_submissions_df['selftext'].isna() | (text_submissions_df['selftext'] == '')].index, inplace=True)\n",
    "# text_submissions_df.dropna(subset=['title'], inplace=True) # Drop na or empty titles\n",
    "\n",
    "# Drop this row because it has an invalid date and other columns are nan\n",
    "text_submissions_df=text_submissions_df[~(text_submissions_df['created_utc'] == \"CPTSD\")]\n",
    "\n",
    "text_submissions_df['created_utc'] = text_submissions_df['created_utc'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bot detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (Q): Bot Detection not necessary for Posts? just for text_comments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating data to pre-pandemic and pandemic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start of the pandemic as of February 1, 2020\n",
    "# created_at is a UNIX timestamp. Feb 1, 2020 00:00:00 UTC = 1580515200\n",
    "start_of_pandemic = 1580515200\n",
    "text_submissions_df_pre = text_submissions_df[text_submissions_df[\"created_utc\"] < start_of_pandemic]\n",
    "text_submissions_df_post = text_submissions_df[text_submissions_df[\"created_utc\"] >= start_of_pandemic]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the 4 updated cvs which have cleaned up the data and split the comments and submissions csv files into 2 \n",
    "# because they are smaller files, thus easier to load up and deal with\n",
    "# text_submissions_df_pre.to_csv('data/text_submissions_pre.csv')\n",
    "# text_submissions_df_post.to_csv(\"data/text_submissions_post.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detecting COVID related posts\n",
    "We detect submissions related to covid by searching in the their title for covid related words (with no case sensitivity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>domain</th>\n",
       "      <th>is_self</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>t3_krer7q</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1609902052</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>False</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>James Charles calls out youtuber JennxPenn for...</td>\n",
       "      <td>BeautyGuruChatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>t3_fjm72g</td>\n",
       "      <td>ButterscotchFog</td>\n",
       "      <td>1584370462</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shout out to creators whose content continues ...</td>\n",
       "      <td>BeautyGuruChatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>t3_fko0g4</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1584530758</td>\n",
       "      <td>i.redd.it</td>\n",
       "      <td>False</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>(UPDATE) Mykie/GlamandGore addressing her firs...</td>\n",
       "      <td>BeautyGuruChatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>t3_jzy1ls</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1606191804</td>\n",
       "      <td>self.BeautyGuruChatter</td>\n",
       "      <td>True</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>I’m just Curious as to why BGs, celebs, models...</td>\n",
       "      <td>BeautyGuruChatter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>t3_j1dj17</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1601305059</td>\n",
       "      <td>jezebel.com</td>\n",
       "      <td>False</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>Jeffree Star Took Covid-19 Loans, Months After...</td>\n",
       "      <td>BeautyGuruChatter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id           author  created_utc                  domain is_self  \\\n",
       "21   t3_krer7q        [deleted]   1609902052              reddit.com   False   \n",
       "44   t3_fjm72g  ButterscotchFog   1584370462               i.redd.it   False   \n",
       "45   t3_fko0g4        [deleted]   1584530758               i.redd.it   False   \n",
       "103  t3_jzy1ls        [deleted]   1606191804  self.BeautyGuruChatter    True   \n",
       "126  t3_j1dj17        [deleted]   1601305059             jezebel.com   False   \n",
       "\n",
       "      selftext                                              title  \\\n",
       "21   [deleted]  James Charles calls out youtuber JennxPenn for...   \n",
       "44         NaN  Shout out to creators whose content continues ...   \n",
       "45   [deleted]  (UPDATE) Mykie/GlamandGore addressing her firs...   \n",
       "103  [removed]  I’m just Curious as to why BGs, celebs, models...   \n",
       "126  [deleted]  Jeffree Star Took Covid-19 Loans, Months After...   \n",
       "\n",
       "             subreddit  \n",
       "21   BeautyGuruChatter  \n",
       "44   BeautyGuruChatter  \n",
       "45   BeautyGuruChatter  \n",
       "103  BeautyGuruChatter  \n",
       "126  BeautyGuruChatter  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "covid_related_words = ['pandemic', 'covid']\n",
    "search_pattern = '|'.join(covid_related_words)\n",
    "covid_submissions = text_submissions_df[text_submissions_df['title'].str.contains(search_pattern, case=False)]\n",
    "\n",
    "# printing the title of covid related posts\n",
    "covid_submissions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'label': 'sadness', 'score': 0.007511448580771685},\n",
       "  {'label': 'joy', 'score': 0.8325576186180115},\n",
       "  {'label': 'love', 'score': 0.014407153241336346},\n",
       "  {'label': 'anger', 'score': 0.13556727766990662},\n",
       "  {'label': 'fear', 'score': 0.008320360444486141},\n",
       "  {'label': 'surprise', 'score': 0.0016362066380679607}],\n",
       " [{'label': 'sadness', 'score': 0.048631247133016586},\n",
       "  {'label': 'joy', 'score': 0.007470840588212013},\n",
       "  {'label': 'love', 'score': 0.03859551250934601},\n",
       "  {'label': 'anger', 'score': 0.902003288269043},\n",
       "  {'label': 'fear', 'score': 0.0024381077382713556},\n",
       "  {'label': 'surprise', 'score': 0.00086104596266523}]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample - delete later\n",
    "classifier = pipeline(\"text-classification\",model='bhadresh-savani/distilbert-base-uncased-emotion', return_all_scores=True)\n",
    "prediction = classifier([\"I love using transformers.\", \"I hate you so much\"],)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average submission sentiment score pre-pandemic\n",
    "We want to find the average score (0-1) for each sentiment (sadness, joy, love, anger, fear, surprise) for submissions and comments for posts pre-pandemic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6347/547304523.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  text_submissions_df_pre_subset[emotion] = float('NaN')\n",
      "Processing: 4252.75%: 100%|██████████| 33518/33518 [16:04<00:00, 34.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO for now we do it for submissions - figure it out for comments as well? - takes multiple hours\n",
    "# TODO even for submissions selftext it's taken 10-11 hours.. so just doing it with title right now\n",
    "# TODO also there's a limit to the characters for this classifier\n",
    "\n",
    "# for now take 5% of text_submissions_df_pre (100% of the data takes >10 hours)\n",
    "ten_percent = int(len(text_submissions_df_pre) * 0.05)\n",
    "text_submissions_df_pre_subset = text_submissions_df_pre.head(ten_percent)\n",
    "\n",
    "# add new columns in submissions dataframe for each emotion and set all to NaN\n",
    "emotions = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "for emotion in emotions:\n",
    "    text_submissions_df_pre_subset[emotion] = float('NaN')\n",
    "\n",
    "def get_scores_and_update(row):\n",
    "    scores = classifier(row['title'])[0]\n",
    "    for score in scores:\n",
    "        emotion = score['label']\n",
    "        row[emotion] = score['score']\n",
    "    return row\n",
    "  \n",
    "# extract score for each emotion and set the score to the particular column\n",
    "total_rows = len(text_submissions_df_pre_subset)\n",
    "with tqdm(total=total_rows) as pbar:\n",
    "  for index, row in text_submissions_df_pre_subset.iterrows():\n",
    "      scores = classifier(row['title'])[0]\n",
    "      for score in scores:\n",
    "          emotion = score['label']\n",
    "          text_submissions_df_pre_subset.at[index, emotion] = score['score']\n",
    "      \n",
    "      pbar.update(1)\n",
    "      pbar.set_description(f'Processing: {((index + 1) / total_rows) * 100:.2f}%')  # Display progress percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average sadness score: 0.08333218761260267\n",
      "average joy score: 0.39390795732316913\n",
      "average love score: 0.014010731028408201\n",
      "average anger score: 0.35626362880260765\n",
      "average fear score: 0.14184019473937085\n",
      "average surprise score: 0.0106453003270222\n"
     ]
    }
   ],
   "source": [
    "for emotion in emotions:\n",
    "  print('average', emotion, 'score:', text_submissions_df_pre_subset[emotion].mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
